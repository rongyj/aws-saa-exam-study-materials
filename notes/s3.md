# S3

* [S3 FAQs](https://aws.amazon.com/s3/faqs/)

- S3 101

    ## S3: Simple Storage Solution

    S3 provides developers and IT teams with secure, durable, highly-scalable object storage. Amazon S3 is easy to use, with a simple web services interface to store and retrieve any amount of data from anywhere on the web.

    Basically S3 is a safe place to store your files.

    Data is spread across multiple devices and facilities

    S3 is object-based storage which just means that it lets you upload files.

    Files can be from 0 Bytes to 5TB.

    Storage is unlimited

    Files are stored in "buckets"

    S3 is a universal namespace — names must be unique globally

    This is because it's going to create a web address.

    If you made a bucket called spongebob-memes in N. Virginia, your buckets url would be:

    https://spongebob-memes.s3.amazonaws.com/

    If you made your bucket in Ireland, it would be at

    https://spongebob-memes.eu-west-1.amazonaws.com/

    When you upload a file to S3, you will receive an HTTP 200 code if the upload is successful

    Objects consist of the following:

    Key: name of the object

    Value: the data that is the object

    Version ID: important versioning

    Metadata: data about the data you're storing

    Subresources:

    Access Control Lists

    Torrents

    How does data consistency work for S3?

    Read after Write consistency for PUTS of new objects

    This just means you are able to read immediately after writing the new object

    Eventual consistency for overwrite PUTS and DELETES (can take some time to propagate)

    If you upload v2 of a file and try to immediately read, you may or may not get v2. Generally If you wait at least a couple seconds, you will get v2. So, you'll eventually get the correct version is you overwrite

    What are the guarantees from Amazon?

    Built for 99.99% availability for the S3 platform

    Amazon Guarantees 99.9% availability

    11 x 9s durability for S3 information (99.999999999%)

    What are S3 features:

    Lifecycle Management

    Move objects around different tiers

    Versioning

    Encryption 

    MFA Delete

    Secure your data using Access Control Lists and Bucket Policies

    Tiered Storage Available

    Storage Classes:

    S3 Standard: 

    99.99% availability 

    99.999999999% durability

    Stored redundantly across multiple devices in multiple facilities

    Designed to sustain the loss of 2 facilities concurrently

    S3-IA:

    IA — Infrequent Access

    For data that is accessed less frequently, but requires rapid access when needed

    Lower fee than S3, but you are charged a retrieval fee

    S3 One Zone IA:

    For when you want even lower cost option

    No need for multiple availability zone resilience

    S3 Intelligent Tiering:

    Design to optimize costs by automatically moving data to the most cost effective tier without performance impact or operational overhead

    S3 Glacier:

    Secure, durable, low cost storage class for data archives

    Reliable storage at any volume

    Cheaper than or competitive with on-prem solutions

    Retrieval time is configurable from minutes to hours

    S3 Glacier Deep Archive:

    S3 Glacier Deep Archive is S3s lowest-cost storage class where a retrieval time of 12 hours is acceptable

    ![S3%20b25a5e54d09741e09322055105ca2345/Screen_Shot_2020-04-07_at_7.58.51_PM.png](S3%20b25a5e54d09741e09322055105ca2345/Screen_Shot_2020-04-07_at_7.58.51_PM.png)

    **first byte latency is how quickly you can retrieve your data

    S3 Billing is calculated by:

    Storage

    Requests

    Storage Management Pricing (Tiers)

    Data Transfer Pricing

    Transfer Acceleration

    Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your end users and an S3 bucket. Takes advantage of Cloudfront's globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.

    Cross Region Replication

    Will automatically replicate objects from upload region to other region for HA/DR

    S3 Exam Tips:

    S3 is object based (think files)

    Files can be from 0B to 5TB

    Unlimited storage

    Files are stored in buckets

    S3 is a universal namespace

    Not suitable to install an OS or DB on

    Successful uploads generate HTTP 200 status code

    MFA Delete requires MFA to delete objects — you need to turn this on

    Objects:

    Key
    Value

    Version ID

    Metadata

    Subresources

    Access Control Lists

    Torrent

    Read after Write Consistency for PUTS of *new* objects

    Eventual Consistency for overwrite PUTS or DELETES

    Storage Tiers:

    S3 Standard

    S3-IA

    S3 One Zone - IA

    S3 Intelligent Tiering

    S3 Glacier

    S3 Glacier Deep Archive

    Read the S3 FAQs

- S3 Pricing Tiers

    ### What makes up the cost of S3?

    - Storage
    - Requests and Data Retrievals
    - Data Transfer
    - Management and Replication

    ### Tiers

    - S3 Standard and Intelligent Tiering (prices based on N. Virginia)

        ![S3%20b25a5e54d09741e09322055105ca2345/Screen_Shot_2020-04-12_at_12.55.45_PM.png](S3%20b25a5e54d09741e09322055105ca2345/Screen_Shot_2020-04-12_at_12.55.45_PM.png)

    - S3 Infrequently Accessed, One Zone IA, Glacier, and Glacier Deep Archive

        ![S3%20b25a5e54d09741e09322055105ca2345/Screen_Shot_2020-04-12_at_12.57.37_PM.png](S3%20b25a5e54d09741e09322055105ca2345/Screen_Shot_2020-04-12_at_12.57.37_PM.png)

    ### How to get best value from S3?

    - S3 Standard
    - S3 IA
    - S3 Intelligent Tiering — gives you cost optimization between S3 Standard and S3 IA
    - S3 One Zone IA (if you don't need redundancy)
    - S3 Glacier (only suitable for archives)
    - S3 Glacier Deep Archive (only suitable for archives)
- S3 Security and Encryption
    - By default all newly created buckets are *private.*
    - You can set up access control to buckets using Bucket Policies or Access Control lists
    - S3 buckets can be configured to create access logs which log all requests made to the S3 bucket. This can be sent to another bucket (even if it's in a different account).

    ### Different Types of Encryption:

    - Encryption in Transit
        - Achieved by SSL/TLS (think https)
    - Encryption at Rest (stored encrypted)
        - Server Side Encryption (SSE) at Rest
            - S3 Managed Keys (SSE-S3)
            - AWS Key Management Service (SSE-KMS)
            - Customer Provided Keys (SSE-C)
        - Client Side Encryption at Rest
            - You encrypt client side, and upload encrypted object to S3
- S3 Version Control

    ### What can we do with versioning?

    - Stores all versions of an object (including all writes and even if you delete an object)
    - Great backup tool
    - Once enabled it cannot be disabled, only suspended
    - Integrates with Lifecycle Rules
    - Comes with MFA Delete capability — to *permanently* delete something, you would need to authenticate with MFA.
    - Uploading a new version changes permissions on the file
    - Hide/Show Versions toggle
    - The size of your S3 bucket includes the multiple versions of a file. So if you have a file with 2 versions, version 1 is 66kb and version 2 is 94kb, your S3 bucket is 160kb. You can use lifecycle policies to help with this.
    - You can hide versions and do a delete. This will make it look like the bucket is empty unless you toggle version showing again. Here you will see your n versions plus one more noted as the delete marker. If you delete the delete marker, all goes back to the way it was. Deleting a version, will hard delete the version and set the previous to the latest version.
- S3 Lifecycle Management and Glacier
    - Adding prefix or tag filter means that only the objects with that tag will be affected by the lifecycle rule
    - Exam Tips
        - Automate moving your objects between different storage tiers
        - Can be used in conjunction with versioning
        - Can be applied to current and previous versions
- AWS Organizations and Consolidated Billing

    ### What is AWS Organizations?

    - AWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage.
    - OU — Organizational Unit
    - Best Practices: Don't deploy any resources in root
    - Consolidated Billing — Paying account is independent. Cannot access resources of other accounts. You have linked accounts that are independent. One bill per AWS account, easy to track charges and allocate costs, and you get volume pricing discount. Tracks costs across all accounts.
    - Exam Tips:
        - always enable MFA on root account
        - Paying account should be used for billing purposes only
        - Enable/disable services using Service Control PoliCies either on OU or on individual accounts
- Sharing S3 Buckets Across Accounts
    - 3 ways to share access across accounts:
        - Using Bucket Policies and IAM (applies across the entire bucket. This only allows programmatic access.
        - Using Bucket ACLs (Access Control Lists) & IAM (applies to individual objects). Programmatic access only.
        - Cross-Account IAM Roles. Programmatic AND console access.
- S3 Cross Region Replication
    - When you initialize CRR for a bucket, the objects already in that bucket are not copied over. You need to do this "manually".
    - Objects added after CRR is turned on will be replicated
    - Delete markers are not replicated
    - Individual version deletes are not replicated
    - Versioning must be enabled on both source and destination buckets
- S3 Transfer Accelleration
    - What is S3 Transfer Acceleration?
        - Utilizes CloudFront Edge network to accelerate uploads to S3. Instead of uploading directly to your S3 bucket, you can use a distinct URL to upload directly to an edge location which will then transfer that file to S3. You will get a distinct URL to upload to.
        - There is a S3 acceleration test tool
- CloudFront
    - What is CloudFront
        - CloudFront is a CDN. A CDN is a system of distributed servers (a network) that deliver webpages and other web content to a user based on the geographic locations of the user, the origin of the webpage, and a content delivery server.
            - Basically the benefit here is that if your servers are in London and you don't have a CDN enabled for your content, someone is Austrailia is going to see poor TTFB due to data having to travel across the whole world.
            - CloudFront is a global service
        - Key Terms with CloudFront:
            - Edge Location: This is a location where content will be cached. This is separate to an AWS Region/AZ
            - Origin: This is the origin of all the files that CDN will distribute. This can be an S3 Bucket, an EC2 instance, an Elastic Load Balancer, or Route53
            - Distribution: This is the name given the DN which consists of a collection fo Edge Locations.
        - How does CloudFront work?
            - Basically, if your files/services live on a server in London, and a user in a different part of the world requests it from their edge location, and that location doesn't have the content cached, it will download it from the origin server and cache it for the TTL. If anyone else queries the same edge location for that content, the edge location will serve it up without querying the origin server. This will result in a potentially drastic increase in performance for subsequent users because data doesn't have to traverse massive distances.
            - Can be used to deliver entire websites, including dynamic, static, streaming, and interactive content using a global network of edge locations. Requests for your content are automatically routed to the nearest edge location, so content is delivered with the best possible performance.
            - Two different types of distributions:
                - RTMP: used for media streaming
                - Web Distribution: typically used for websites
            - Exam Tips:
                - Edge location definition
                - Origin definition
                - Distribution definition
                - Web Distribution vs RTMP
                - Edge locations are not read only. You can write, too.
                - Objects are cached for TTL
                - Cache invalidation is possible — but you will be charged to do so.
                - You can restrict access to URLs using signed URLs or signed cookies
- Snowball
    - What is Snowball?
        - Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of AWS. Using Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns. Transferring data with snowball is simple, fast, secure, and can be as little as one-fifth the costs of high-speed internet.
        - Snowball comes in either a 50TB or 80TB size. Snowball uses multiple layers of security designed to protect your data including tamper-resistant enclosures, 256-bit encryption, and an industry-standard Trusted Platform Module (TPM) designed to ensure both security and full chain-of-custody of your data. Once the data transfer job has been processed and verified, AWS performs a software erasure of the Snowball appliance. This erasure is done in such a way that previous data is totally unrecoverable.
        - Snowball Edge is a 100TB appliance with on-board compute capabilities. You can use  Snowball Edge to move large amounts of data into and out of AWS, as a temporary storage tier for large local datasets, or to support local workloads in remote of offline locations.
            - An airline company uses AWS Snowball Edges on their craft to store data about flights and run lambdas on that data.
            - Snowball Edge connects to your existing applications and infrastructure using standard storage interfaces, streamlining the data transfer process and minimizing setup and integration.
            - Snowball Edge can cluster together ot form a local storage tier and process your data on-premises, helping ensure your applications continue to run even when they are not able to access the cloud.
        - Snowmobile is an exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile. A Snowmobile is a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. Snowmobile makes it easy to move massive volumes of data into the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is secure, fast and cost effective.
- Storage Gateway
    - AWS Storage Gateway is a service that connects an on-premises software appliance with cloud-based storage to provide seamless and secure integration between an organizations on-premisesIT environment and AWSs storage infrastructure. The service enables you to securely store data to the AWS cloud for scalable and cost-effective storage.
    - AWS Storage Gateway's  software appliance is available for download as a VM image that you install on a host in your datacenter. Storage Gateway supports either VMware ESXi or Microsoft Hyper-V. Once your've installed your gateway and associated it with your AWS account through the activation process, you can use the AWS Management Console to create the storage gateway option that is right for you.
    - Three different types of Storage Gateway:
        - File Gateway (NFS & SMB)
            - Files are stored as objects in your S3 buckets, accessed through a Network File System mount point. Ownership, permissions, and timestamps are durably stored in S3 in the user-metadata of the object associated with the file. Once objects are transferred to S3, they can eb managed as native S3 objects, and bucket policies such as versioning, lifecycle management, and cross-region replication apply directly to the objects stored in your bucket.
        - Volume Gateway (iSCSI)
            - Stored Volumes
            - Cached Volumes
            - The Volume interface presents your applications with disk volumes using the iSCSI block protocol.
            - Data written to these volumes can be asynchronously backed up as point-in-time snapshots of your volumes, and stored in the cloud as Amazon EBS snapshots
            - Snapshots are incremental backups that capture only changed blocks. All snapshot storage is also compressed to minimize your storage costs.
            - Stored Volumes let you store your primary data locally, while asynchronously backing up that data to AWS. Stored volumes provide your on-premises applications with low-latency access to their entire datasets, while providing durable offsite backups. You can create storage volumes and mount them as iSCSI devices from your on-premises application servers. Data written to your stored volumes is stored on your on-remises storage hardware. This data is asynchronously backed up to S3 in the form of EBS snapshots. Stored volumes can be 1GB-16TB in size.
            - Cached volumes let you use S3 as your primary data storage while retaining frequently accessed data locally in your storage gateway. Cached volumes minimize the need to scale your on-premises storage infrastructure, while still providing your applications with low-latency access to their frequently accessed data. You can create storage volumes up to 32TiB in size and attach to them as iSCSI devices from your on-premises application servers. Your gateway stores data that write to these volumes in Amazon S3 and retains recently read data in your on-premises storage gateway's cache and upload buffer storage. 1GB-32TB in size for cached volumes.
        - Tape Gateway (VTL)
            - Tape Gateway offers durable, cost-effective solution to archive your data in the AWS Cloud. The VTL interface it provides lets your leverage your existing tape-based backup application infrastructure to store data on virtual tape cartridges that you create on your tape gateway. Each tape gateway is preconfigured with a media changer and tape drives, which are available to your existing client backup applications as iSCSI devices. You add tape cartridges as your need to archive your data. Supported by NetBackup, Backup Exec, Veeam, Etc.
    - Exam Tips
        - File Gateway
        - Volume Gateway
            - stored vs cached volumes
        - Tape Gateway
- Athena vs Macie
    - Athena
        - Athena is an interactive query service taht lets you analyse and query data located in S3 using standard SQL.
            - It is serverless, so nothing to provision, and you pay per query / per TB scanned
            - No need to set up complex ETL processes
            - Works directly with data stored in S3
            - Good for querying log files stored in S3, generate business reports on data stored in S3, analyze cost and usage reports, run queries on click-stream data
    - Macie
        - Macie is a security service which uses ML and NLP to discover, classify, and protect sensitive data stored in S3.
            - Uses AI to recognize if your S3 objects contain sensitive data such as PII
            - Dashboards, reporting and alerts
            - Works directly with data stored in S3
            - Can also analyze CloutTrail logs
            - Create for PCI-DSS and preventing ID theft

is there a way to propagate deletes across buckets?

No way to override AWS default of not replicating deletes. Ansible delete API requires a bucket name, so the best way to automate object deletion is to provide all bucket names to the playbook and have it delete the specified object in each bucket.

S3 used to have IOPS limits of 100 PUTs. As of July 2018, this was raised to 3500, which basically eliminates the need for Key design.

Multipart Upload API for large files